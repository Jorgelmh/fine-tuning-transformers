{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318c960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "from torchtext.vocab import GloVe, Vocab\n",
    "from collections import Counter\n",
    "from conceptnet import get_conceptnet_facts_for_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20617241",
   "metadata": {},
   "source": [
    "## Common Sense Knowledge Retrieval (ConceptNet)\n",
    "\n",
    "In this section, we define the helper functions used to fetch and process **common sense knowledge** from **ConceptNet**.\n",
    "\n",
    "Given a set of object concepts (e.g. extracted from MS COCO annotations or an object detector), these functions:\n",
    "- query ConceptNet for relevant relations,\n",
    "- filter and rank the retrieved facts,\n",
    "- and format them into a compact representation suitable for use as model input.\n",
    "\n",
    "The resulting knowledge will later be combined with visual features to augment image caption generation with external commonsense information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbd8145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_conceptnet_facts_for_image([\"dog\", \"park\", \"ball\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a69bcd",
   "metadata": {},
   "source": [
    "## Load GloVe and Build a Vocab\n",
    "\n",
    "Here we:\n",
    "\n",
    "1. Load pretrained GloVe vectors using `torchtext.vocab.GloVe`.\n",
    "2. Build a `Vocab` object from `glove.stoi` (string-to-index mapping).\n",
    "3. Add special tokens:\n",
    "   - `<unk>` for unknown words\n",
    "   - `<pad>` for padding\n",
    "\n",
    "The important idea:\n",
    "\n",
    "> **GloVe is just a big lookup table.**\n",
    "\n",
    "Each word has a fixed vector, and we wrap that into a PyTorch `nn.Embedding` layer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe\n",
    "glove = GloVe(name=\"6B\", dim=100)\n",
    "\n",
    "# Define special tokens\n",
    "specials = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "# Build a Counter from GloVe vocab (all frequency=1)\n",
    "counter = Counter(glove.stoi.keys())\n",
    "\n",
    "# Create Vocab properly\n",
    "my_vocab = Vocab(counter, specials=specials)\n",
    "\n",
    "vocab_size = len(my_vocab)\n",
    "embedding_dim = glove.dim  # same as glove.vectors.size(1)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Embedding dim:\", embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2a420",
   "metadata": {},
   "source": [
    "## Create an Embedding Layer from Pretrained Vectors\n",
    "\n",
    "We now wrap the GloVe tensor into an `nn.Embedding` using\n",
    "`nn.Embedding.from_pretrained`.\n",
    "\n",
    "- `glove.vectors` is a tensor of shape `[vocab_size_without_specials, embedding_dim]`.\n",
    "- We need to **extend** it to include our `<unk>` and `<pad>` rows.\n",
    "- `freeze=True` means we do **not** train the embeddings; they stay as GloVe.\n",
    "\n",
    "This layer is still just a **lookup table**: it maps token IDs → word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a weight matrix that matches our vocab (including specials)\n",
    "num_specials = len(specials)\n",
    "pad_vectors = torch.zeros(num_specials, embedding_dim)\n",
    "\n",
    "# Order: specials first, then GloVe vectors\n",
    "embedding_weights = torch.cat([pad_vectors, glove.vectors], dim=0)\n",
    "assert embedding_weights.size(0) == vocab_size\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    embedding_weights,\n",
    "    freeze=True  # set to False if you want to fine‑tune the embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ced10",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Self‑attention by itself is **position‑agnostic**. It doesn't know which token\n",
    "came first.\n",
    "\n",
    "We add a standard sinusoidal positional encoding (as in the original Transformer paper):\n",
    "\n",
    "- Same `d_model` as the embeddings\n",
    "- Precomputed for a maximum sequence length\n",
    "- Added to the embeddings before passing them to the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cca8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.cos(position * div_term)\n",
    "        pe[:, 1::2] = torch.sin(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional encodings to input.\n",
    "\n",
    "        x shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60978e3d",
   "metadata": {},
   "source": [
    "## Model Architecture: Dual-Encoder + Single Decoder (Image + Commonsense)\n",
    "\n",
    "We use a **dual-encoder, single-decoder** architecture to generate captions conditioned on:\n",
    "1) the **image content**, and  \n",
    "2) a compact set of **commonsense facts** retrieved from ConceptNet (derived from the objects in the image).\n",
    "\n",
    "### High-level idea\n",
    "- The **vision encoder** converts the image into a sequence of **image tokens**.\n",
    "- The **knowledge encoder** converts the retrieved commonsense facts into a sequence of **knowledge tokens**.\n",
    "- A **caption decoder** generates the caption autoregressively while attending to **both** token sequences via cross-attention.\n",
    "\n",
    "---\n",
    "\n",
    "### Components\n",
    "\n",
    "**1) Vision Encoder (ViT)**\n",
    "- Input: an RGB image (e.g., `256×256×3`)\n",
    "- The image is split into fixed-size patches (e.g., `16×16`), projected into an embedding space, and processed by a Transformer encoder.\n",
    "- Output: `V_img ∈ R^{N_img × d}` (a sequence of image token embeddings)\n",
    "\n",
    "**2) Knowledge Encoder (Text Transformer)**\n",
    "- Input: a list of ConceptNet facts converted into short natural-language sentences (e.g., `\"A frisbee is used for play.\"`)\n",
    "- Facts are tokenized and embedded, then processed by a Transformer encoder.\n",
    "- Output: `V_kg ∈ R^{N_kg × d}` (a sequence of knowledge token embeddings)\n",
    "\n",
    "**3) Caption Decoder (Transformer)**\n",
    "- Input: caption tokens generated so far (teacher forcing during training)\n",
    "- The decoder performs:\n",
    "  - self-attention over caption tokens\n",
    "  - cross-attention to `V_img` (visual grounding)\n",
    "  - cross-attention to `V_kg` (commonsense augmentation)\n",
    "- Output: next-token distribution over the caption vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-attention fusion (two memories)\n",
    "At each decoder layer, we attend to two separate memories:\n",
    "- **Image memory:** keys/values from `V_img`\n",
    "- **Knowledge memory:** keys/values from `V_kg`\n",
    "\n",
    "A common implementation is **sequential cross-attention**:\n",
    "1. cross-attend to image tokens (grounding)\n",
    "2. cross-attend to knowledge tokens (augmentation)\n",
    "\n",
    "This keeps image evidence primary while allowing knowledge to refine the generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b33bad",
   "metadata": {},
   "source": [
    "--------\n",
    "## ViT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f12720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT-style image encoder (patch embeddings + Transformer encoder).\n",
    "\n",
    "    Key detail: we use a Conv2d as a *patch embedding* layer.\n",
    "    - kernel_size = patch_size and stride = patch_size\n",
    "    - so each convolution “step” looks at one non-overlapping patch (e.g. 16x16)\n",
    "    - and produces a d_model-dimensional vector for that patch\n",
    "    This is equivalent to: flatten(patch) -> Linear(patch_dim -> d_model),\n",
    "    but implemented efficiently as a convolution.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 256,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        d_model: int = 768,\n",
    "        n_layers: int = 6,\n",
    "        n_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.1,\n",
    "        use_cls_token: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.d_model = d_model\n",
    "        self.use_cls_token = use_cls_token\n",
    "\n",
    "        grid = image_size // patch_size\n",
    "        self.num_patches = grid * grid\n",
    "\n",
    "        # Patch embedding via Conv2d:\n",
    "        # Input:  (B, 3, 256, 256) 256x256 pixel RGB image\n",
    "        # Output: (B, d_model, 16, 16)  when patch_size=16\n",
    "        # Each (16,16) spatial location corresponds to one patch and has a d_model-dimensional embedding vector (these are convolution filter outputs)\n",
    "        # The idea here is to project the image patches into d_model-dimensional tokens for the Transformer.\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        seq_len = self.num_patches + (1 if use_cls_token else 0)\n",
    "\n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "\n",
    "        # Learned absolute positional embedding for each token in the sequence\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            batch_first=True,   # (B, T, C)\n",
    "            norm_first=True,    # pre-norm\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        if self.cls_token is not None:\n",
    "            nn.init.normal_(self.cls_token, std=0.02)\n",
    "        nn.init.kaiming_normal_(self.patch_embed.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if self.patch_embed.bias is not None:\n",
    "            nn.init.zeros_(self.patch_embed.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W) with H=W=image_size\n",
    "        returns: (B, T, d_model) where T = num_patches (+1 if CLS)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.image_size and W == self.image_size, \\\n",
    "            f\"Expected {self.image_size}x{self.image_size}, got {H}x{W}\"\n",
    "\n",
    "        # 1) Patch embedding\n",
    "        # (B, d_model, grid, grid)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # 2) Flatten spatial grid -> sequence of tokens\n",
    "        # flatten(2): (B, d_model, grid*grid) then transpose -> (B, grid*grid, d_model)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, d_model)\n",
    "\n",
    "        # 3) Optional CLS token\n",
    "        if self.use_cls_token:\n",
    "            cls = self.cls_token.expand(B, -1, -1)      # (B, 1, d_model)\n",
    "            x = torch.cat([cls, x], dim=1)              # (B, 1+num_patches, d_model)\n",
    "\n",
    "        # 4) Add positional embeddings -> element wise addition of learned weights\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # 5) Transformer encoder\n",
    "        x = self.encoder(x)  # (B, T, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89e32c",
   "metadata": {},
   "source": [
    "## Knowledge Encoder\n",
    "\n",
    "Encodes the facts from conceptnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeTextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes commonsense facts (already tokenized) into a sequence of embeddings.\n",
    "\n",
    "    Inputs:\n",
    "      - input_ids: (B, T_kg)\n",
    "      - attention_mask: (B, T_kg) with 1 for keep, 0 for pad\n",
    "\n",
    "    Output:\n",
    "      - V_kg: (B, T_kg, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_layer: nn.Embedding,\n",
    "        d_model: int = 768,\n",
    "        n_layers: int = 4,\n",
    "        n_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.1,\n",
    "        pad_token_id: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.token_embed = embedding_layer\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=int(d_model * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,  # pre-norm\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # input_ids: (B, T)\n",
    "        x = self.token_embed(input_ids)  # (B, T, d)\n",
    "\n",
    "        # Add positional encodings\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687abf1d",
   "metadata": {},
   "source": [
    "## Dual-memory decoder (self-attn + cross-attn to image + cross-attn to KG)\n",
    "\n",
    "A single decoder layer (custom, because nn.TransformerDecoderLayer only supports one memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualCrossAttnDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One decoder layer with:\n",
    "      1) causal self-attention over caption tokens\n",
    "      2) cross-attention to image tokens\n",
    "      3) cross-attention to knowledge tokens\n",
    "      4) FFN\n",
    "\n",
    "    Pre-norm design for stability.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 768,\n",
    "        n_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn_img = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn_kg = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.norm4 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        hidden = int(d_model * mlp_ratio)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,                         # (B, T_txt, d)\n",
    "        img_mem: torch.Tensor,                   # (B, T_img, d)\n",
    "        kg_mem: torch.Tensor,                    # (B, T_kg, d)\n",
    "        *,\n",
    "        txt_key_padding: torch.Tensor = None,    # (B, T_txt) True=pad\n",
    "        img_key_padding: torch.Tensor = None,    # (B, T_img) True=pad (often None)\n",
    "        kg_key_padding: torch.Tensor = None,     # (B, T_kg) True=pad\n",
    "        causal_mask: torch.Tensor = None,        # (T_txt, T_txt) True=block\n",
    "    ) -> torch.Tensor:\n",
    "        # 1) Masked causal self-attention -> only look at previous tokens\n",
    "        h = self.norm1(x)\n",
    "        attn, _ = self.self_attn(\n",
    "            h, h, h,\n",
    "            attn_mask=causal_mask,\n",
    "            key_padding_mask=txt_key_padding,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        x = x + self.drop(attn)\n",
    "\n",
    "        # 2) cross-attention to image memory\n",
    "        h = self.norm2(x)\n",
    "        attn, _ = self.cross_attn_img(\n",
    "            h, img_mem, img_mem,\n",
    "            key_padding_mask=img_key_padding,\n",
    "            need_weights=False,\n",
    "        )\n",
    "\n",
    "        # Add & Norm\n",
    "        x = x + self.drop(attn)\n",
    "        h = self.norm3(x)\n",
    "\n",
    "        # 3) cross-attention to knowledge memory\n",
    "        attn, _ = self.cross_attn_kg(\n",
    "            h, kg_mem, kg_mem,\n",
    "            key_padding_mask=kg_key_padding,\n",
    "            need_weights=False,\n",
    "        )\n",
    "\n",
    "        # Add & Norm\n",
    "        x = x + self.drop(attn)\n",
    "        h = self.norm4(x)\n",
    "\n",
    "        # 4) FFN\n",
    "        x = x + self.drop(self.ffn(h))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoderCaptionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoregressive caption decoder that attends to two encoder memories (image + knowledge).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_layer: nn.Embedding,\n",
    "        d_model: int = 768,\n",
    "        n_layers: int = 6,\n",
    "        n_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        dropout: float = 0.1,\n",
    "        pad_token_id: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.token_embed = embedding_layer\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DualCrossAttnDecoderLayer(d_model, n_heads, mlp_ratio, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _causal_mask(T: int, device) -> torch.Tensor:\n",
    "        # MultiheadAttention expects True where positions are NOT allowed to attend\n",
    "        return torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,            # (B, T_txt)\n",
    "        attention_mask: torch.Tensor,       # (B, T_txt) 1=keep,0=pad\n",
    "        img_mem: torch.Tensor,              # (B, T_img, d)\n",
    "        kg_mem: torch.Tensor,               # (B, T_kg, d)\n",
    "        kg_attention_mask: torch.Tensor = None,  # (B, T_kg) 1=keep,0=pad\n",
    "        img_attention_mask: torch.Tensor = None, # (B, T_img) (often None/all ones)\n",
    "    ) -> torch.Tensor:\n",
    "        B, T = input_ids.shape\n",
    "        x = self.token_embed(input_ids)  # (B, T, d)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        causal = self._causal_mask(T, input_ids.device)\n",
    "        txt_key_padding = (attention_mask == 0)\n",
    "\n",
    "        kg_key_padding = (kg_attention_mask == 0) if kg_attention_mask is not None else None\n",
    "        img_key_padding = (img_attention_mask == 0) if img_attention_mask is not None else None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                img_mem=img_mem,\n",
    "                kg_mem=kg_mem,\n",
    "                txt_key_padding=txt_key_padding,\n",
    "                img_key_padding=img_key_padding,\n",
    "                kg_key_padding=kg_key_padding,\n",
    "                causal_mask=causal,\n",
    "            )\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa97842",
   "metadata": {},
   "source": [
    "## Full transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a906172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeEnhancedTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_encoder: ViTImageEncoder,\n",
    "        knowledge_encoder: KnowledgeTextEncoder,\n",
    "        caption_decoder: DualEncoderCaptionDecoder,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.knowledge_encoder = knowledge_encoder\n",
    "        self.caption_decoder = caption_decoder\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        pixel_values,        # Raw image pixels (B, 3, H, W)\n",
    "        knowledge_ids,       # ConceptNet fact IDs (B, T_kg)\n",
    "        caption_ids,         # Target caption IDs for teacher forcing (B, T_txt)\n",
    "        caption_mask,        # Mask for captions\n",
    "        knowledge_mask=None  # Mask for knowledge padding\n",
    "    ) -> torch.Tensor:\n",
    "        # 1. Run the Encoders\n",
    "        # Output: (B, T_img, d_model)\n",
    "        img_mem = self.vision_encoder(pixel_values)\n",
    "        \n",
    "        # Output: (B, T_kg, d_model)\n",
    "        kg_mem = self.knowledge_encoder(knowledge_ids)\n",
    "\n",
    "        # 2. Run the Decoder\n",
    "        # The decoder now receives the actual computed features\n",
    "        logits = self.caption_decoder(\n",
    "            input_ids=caption_ids,\n",
    "            attention_mask=caption_mask,\n",
    "            img_mem=img_mem,\n",
    "            kg_mem=kg_mem,\n",
    "            kg_attention_mask=knowledge_mask\n",
    "        )\n",
    "        \n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fine tuning transformers (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
