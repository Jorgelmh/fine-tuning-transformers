{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mini Transformer with Pretrained GloVe Embeddings\n",
        "\n",
        "This notebook is a **reference** for building a simple text classifier using:\n",
        "\n",
        "- Pretrained **GloVe** word embeddings (used as a lookup table)\n",
        "- A **TransformerEncoder** from PyTorch\n",
        "- A final **Linear** layer as classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports\n",
        "\n",
        "We import PyTorch, a few utilities, and `torchtext` for loading GloVe.\n",
        "\n",
        "- `nn.Module`: base class for all neural network modules\n",
        "- `nn.Embedding`: lookup-table layer for vector embeddings\n",
        "- `nn.TransformerEncoder` and `nn.TransformerEncoderLayer`: the core Transformer blocks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.vocab import GloVe, Vocab\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load GloVe and Build a Vocab\n",
        "\n",
        "Here we:\n",
        "\n",
        "1. Load pretrained GloVe vectors using `torchtext.vocab.GloVe`.\n",
        "2. Build a `Vocab` object from `glove.stoi` (string-to-index mapping).\n",
        "3. Add special tokens:\n",
        "   - `<unk>` for unknown words\n",
        "   - `<pad>` for padding\n",
        "\n",
        "The important idea:\n",
        "\n",
        "> **GloVe is just a big lookup table.**\n",
        "\n",
        "Each word has a fixed vector, and we wrap that into a PyTorch `nn.Embedding` layer later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 400002\n",
            "Embedding dim: 100\n"
          ]
        }
      ],
      "source": [
        "# Load GloVe\n",
        "glove = GloVe(name=\"6B\", dim=100)\n",
        "\n",
        "# Define special tokens\n",
        "specials = [\"<unk>\", \"<pad>\"]\n",
        "\n",
        "# Build a Counter from GloVe vocab (all frequency=1)\n",
        "counter = Counter(glove.stoi.keys())\n",
        "\n",
        "# Create Vocab properly\n",
        "my_vocab = Vocab(counter, specials=specials)\n",
        "\n",
        "vocab_size = len(my_vocab)\n",
        "embedding_dim = glove.dim  # same as glove.vectors.size(1)\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Embedding dim:\", embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create an Embedding Layer from Pretrained Vectors\n",
        "\n",
        "We now wrap the GloVe tensor into an `nn.Embedding` using\n",
        "`nn.Embedding.from_pretrained`.\n",
        "\n",
        "- `glove.vectors` is a tensor of shape `[vocab_size_without_specials, embedding_dim]`.\n",
        "- We need to **extend** it to include our `<unk>` and `<pad>` rows.\n",
        "- `freeze=True` means we do **not** train the embeddings; they stay as GloVe.\n",
        "\n",
        "This layer is still just a **lookup table**: it maps token IDs â†’ word vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a weight matrix that matches our vocab (including specials)\n",
        "num_specials = len(specials)\n",
        "pad_vectors = torch.zeros(num_specials, embedding_dim)\n",
        "\n",
        "# Order: specials first, then GloVe vectors\n",
        "embedding_weights = torch.cat([pad_vectors, glove.vectors], dim=0)\n",
        "assert embedding_weights.size(0) == vocab_size\n",
        "\n",
        "embedding_layer = nn.Embedding.from_pretrained(\n",
        "    embedding_weights,\n",
        "    freeze=True  # set to False if you want to fineâ€‘tune the embeddings\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "395ad1fd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.3609, -0.1692, -0.3270,  ...,  0.2714, -0.2919,  0.1611],\n",
              "        [-0.1046, -0.5047, -0.4933,  ...,  0.4253, -0.5125, -0.1705],\n",
              "        [ 0.2837, -0.6263, -0.4435,  ...,  0.4368, -0.8261, -0.1570]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Positional Encoding\n",
        "\n",
        "Selfâ€‘attention by itself is **positionâ€‘agnostic**. It doesn't know which token\n",
        "came first.\n",
        "\n",
        "We add a standard sinusoidal positional encoding (as in the original Transformer paper):\n",
        "\n",
        "- Same `d_model` as the embeddings\n",
        "- Precomputed for a maximum sequence length\n",
        "- Added to the embeddings before passing them to the Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.cos(position * div_term)\n",
        "        pe[:, 1::2] = torch.sin(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add positional encodings to input.\n",
        "\n",
        "        x shape: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Transformer Model (`Net`)\n",
        "\n",
        "We now define a simple classifier model that subclasses `nn.Module`:\n",
        "\n",
        "1. **`__init__`**: define layers\n",
        "   - `self.emb`: our GloVeâ€‘based embedding layer\n",
        "   - `self.pos_encoder`: positional encoding module\n",
        "   - `self.transformer_encoder`: a stack of TransformerEncoderLayers\n",
        "   - `self.classifier`: final linear layer mapping to `num_classes`\n",
        "\n",
        "2. **`forward`**: define the forward pass\n",
        "   - Look up embeddings for token IDs\n",
        "   - Scale them by `sqrt(d_model)`\n",
        "   - Add positional encodings\n",
        "   - Pass through the transformer encoder\n",
        "   - Meanâ€‘pool over sequence length to get a sentence representation\n",
        "   - Pass through classifier to get logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes: int,\n",
        "                 embedding_layer: nn.Embedding,\n",
        "                 nhead: int = 2,\n",
        "                 dim_feedforward: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = embedding_layer\n",
        "        d_model = embedding_layer.embedding_dim\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model=d_model,\n",
        "                                              dropout=dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,  # so input is [batch, seq, d_model]\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        x: tensor of token IDs with shape [batch_size, seq_len]\n",
        "        returns: logits with shape [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        # 1. Embedding lookup (GloVe as lookup table)\n",
        "        x = self.emb(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # 2. Add positional encodings\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # 3. Transformer encoder (selfâ€‘attention + FFN layers)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # 4. Mean pooling over the sequence dimension -> meaning we turn the set of vectors in the sentence into a single average vector\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        # 5. Final classifier\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Simple Tokenization Helper\n",
        "\n",
        "For this reference notebook, we'll use a **very naive tokenizer**:\n",
        "\n",
        "- Lowercase the sentence\n",
        "- Split on spaces\n",
        "- Look up each token in the vocab (unknown words â†’ `<unk>`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([358161, 192974,  43011, 360284, 356532, 325081,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1])\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence: str, vocab_obj, max_len: int = 16) -> torch.Tensor:\n",
        "    tokens = sentence.lower().split()\n",
        "    ids = [vocab_obj[t] for t in tokens[:max_len]]\n",
        "\n",
        "    # Pad if needed\n",
        "    pad_id = vocab_obj[\"<pad>\"]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [pad_id] * (max_len - len(ids))\n",
        "\n",
        "    return torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Quick test\n",
        "example = \"This is a tiny test sentence\"\n",
        "print(encode_sentence(example, my_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Instantiate the Model and Run a Forward Pass\n",
        "\n",
        "Here we create the `Net` model and run a **single forward pass** to see\n",
        "the shapes and verify that everything is wired correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 16, 100])\n",
            "Input IDs shape: torch.Size([2, 16])\n",
            "Logits shape: torch.Size([2, 3])\n",
            "Logits:\n",
            "tensor([[ 0.7014,  0.2494, -0.8791],\n",
            "        [ 0.5865,  0.1323, -0.2063]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 3  # e.g. positive / neutral / negative\n",
        "model = Net(num_classes=num_classes, embedding_layer=embedding_layer)\n",
        "\n",
        "sentence_batch = [\n",
        "    \"This movie was surprisingly good\",\n",
        "    \"I really did not like this\",\n",
        "]\n",
        "\n",
        "batch_ids = torch.stack([\n",
        "    encode_sentence(s, my_vocab, max_len=16) for s in sentence_batch\n",
        "])  # shape: [batch_size, seq_len]\n",
        "\n",
        "logits = model(batch_ids)\n",
        "print(\"Input IDs shape:\", batch_ids.shape)\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "print(\"Logits:\")\n",
        "print(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Recap\n",
        "\n",
        "- **GloVe** provides static word embeddings via a lookup table.\n",
        "- `nn.Embedding.from_pretrained` wraps the lookup table into a PyTorch layer.\n",
        "- `nn.Module` is the base class; `forward()` defines how inputs flow through the model.\n",
        "- `nn.TransformerEncoderLayer` contains selfâ€‘attention + feedâ€‘forward sublayers.\n",
        "- We:\n",
        "  1. Embed tokens\n",
        "  2. Add positional encodings\n",
        "  3. Pass through the Transformer encoder\n",
        "  4. Meanâ€‘pool over time\n",
        "  5. Classify with a linear layer\n",
        "\n",
        "You can now adapt this notebook for your own experiments, add notes, and\n",
        "extend the model as needed. ðŸ’¡\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64327319",
      "metadata": {},
      "source": [
        "## Feed-Forward Network (FFN) in the Transformer\n",
        "\n",
        "In the original *Attention Is All You Need* paper, the Feed-Forward Network (FFN)\n",
        "inside each Transformer encoder layer is defined as:\n",
        "\n",
        "$$\n",
        "\\mathrm{FFN}(x) = \\max(0,\\; xW_1 + b_1)\\, W_2 + b_2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $x$ is a **single token embedding** of dimension $d_{\\text{model}}$\n",
        "- $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$ and $b_1$ expand the representation\n",
        "- $W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$ and $b_2$ compress it back\n",
        "- $\\max(0, \\cdot)$ is the **ReLU activation function**\n",
        "\n",
        "This FFN is applied **independently to each token** (no token-to-token interaction).\n",
        "Token interaction happens in the self-attention block; the FFN is responsible for\n",
        "**non-linear feature transformation** at the token level.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f70dec",
      "metadata": {},
      "source": [
        "## Why the Transformer Uses a Feed-Forward Network\n",
        "\n",
        "The Transformer FFN serves a different role than self-attention:\n",
        "\n",
        "- **Self-attention** mixes information *across tokens*\n",
        "- **Feed-Forward Network (FFN)** transforms information *within each token*\n",
        "\n",
        "The FFN works by:\n",
        "1. Expanding the token embedding into a higher-dimensional space\n",
        "2. Applying a non-linear transformation\n",
        "3. Compressing it back to the original model dimension\n",
        "\n",
        "This pattern gives the model more expressive power while keeping the input/output\n",
        "dimensions compatible with residual connections.\n",
        "\n",
        "### ReLU activation\n",
        "\n",
        "The term\n",
        "\n",
        "$$\n",
        "\\max(0, x)\n",
        "$$\n",
        "\n",
        "is the **ReLU (Rectified Linear Unit)** activation function.\n",
        "\n",
        "ReLU is defined element-wise as:\n",
        "- Output $x$ if $x > 0$\n",
        "- Output $0$ if $x \\le 0$\n",
        "\n",
        "ReLU introduces **non-linearity**, which is essential â€” without it, the FFN would\n",
        "collapse into a single linear transformation and lose expressive power.\n",
        "\n",
        "In PyTorch, the FFN is implemented as:\n",
        "\n",
        "```python\n",
        "x = linear1(x)   # d_model â†’ d_ff\n",
        "x = relu(x)      # non-linearity (max(0, x))\n",
        "x = dropout(x)\n",
        "x = linear2(x)   # d_ff â†’ d_model\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Fine tuning transformers (3.13.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
